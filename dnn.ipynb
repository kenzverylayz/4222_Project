{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
=======
   "execution_count": 2,
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
<<<<<<< HEAD
    "from tensorflow.python.keras.layers import Dense,Input, Dropout, concatenate\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.keras.models import Model\n",
    "# from keras.layers import Input, Dense, Dropout, concatenate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras.optimizers import Adam\n",
=======
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, concatenate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "# Handle missing data for 'overview'\n",
    "df['overview'] = df['overview'].fillna('')\n",
<<<<<<< HEAD
    "df['genres'] = df['genres'].apply(lambda x: x.split('|') if pd.notnull(x) else []) # make genres a list of strings\n",
    "\n",
    "# # Assume genres are stored as strings and handle missing or malformed entries\n",
    "# def process_genres(genre_list):\n",
    "#     if isinstance(genre_list, str):\n",
    "#         return genre_list.split(', ')\n",
    "#     elif isinstance(genre_list, list):\n",
    "#         return genre_list\n",
    "#     return []  # Default case if genres are missing or not a string\n",
    "\n",
    "# df['genres'] = df['genres'].apply(process_genres)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Preprocess the 'overview' text data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Preprocess the 'genres' categorical data, ensuring it remains in a sparse format\u001b[39;00m\n\u001b[0;32m     16\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:248\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    251\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:248\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    251\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "# Preprocess the 'overview' text data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['overview'])\n",
    "\n",
    "# Preprocess the 'genres' categorical data, ensuring it remains in a sparse format\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "genres_matrix = mlb.fit_transform(df['genres'])\n",
    "\n",
    "# Combine 'overview' TF-IDF features with 'genres' binary features, keeping the result sparse\n",
    "combined_features = sp.hstack((tfidf_matrix, genres_matrix), format='csr')\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = train_test_split(combined_features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network architecture\n",
    "input_shape = X_train.shape[1]\n",
    "input_layer = Input(shape=(input_shape,), sparse=True)\n",
    "hidden_layer_1 = Dense(512, activation='relu')(input_layer)\n",
    "dropout_1 = Dropout(0.5)(hidden_layer_1)\n",
    "hidden_layer_2 = Dense(256, activation='relu')(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(hidden_layer_2)\n",
    "output_layer = Dense(input_shape, activation='sigmoid')(dropout_2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train Model - we will use a generator to feed data in batches\n",
    "def batch_generator(X, batch_size):\n",
    "    number_of_batches = X.shape[0] // batch_size\n",
    "    counter = 0\n",
    "    shuffle_index = np.arange(X.shape[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X = X[shuffle_index, :]\n",
    "    while True:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch, :].toarray() # convert to dense\n",
    "        counter += 1\n",
    "        yield X_batch, X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter = 0\n",
    "\n",
    "# Assuming that you cannot fit all the data into memory at once,\n",
    "# you should determine an appropriate batch size for your system.\n",
    "BATCH_SIZE = 256  # Example batch size, adjust to your needs\n",
    "train_generator = batch_generator(X_train, BATCH_SIZE)\n",
    "\n",
    "model.fit(train_generator, epochs=5, steps_per_epoch=X_train.shape[0] // BATCH_SIZE)\n",
    "\n",
    "# ... (rest of the code for making predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.3 GiB for an array with shape (1002267, 12673) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocess the 'genres' categorical data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n\u001b[1;32m----> 7\u001b[0m genres_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Combine 'overview' TF-IDF features with 'genres' binary features in a sparse format\u001b[39;00m\n\u001b[0;32m     10\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m hstack([tfidf_matrix, genres_matrix])\u001b[38;5;241m.\u001b[39mtocsr()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:833\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    830\u001b[0m yt\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inverse[yt\u001b[38;5;241m.\u001b[39mindices], dtype\u001b[38;5;241m=\u001b[39myt\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n\u001b[1;32m--> 833\u001b[0m     yt \u001b[38;5;241m=\u001b[39m \u001b[43myt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m yt\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1056\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1056\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1287\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 47.3 GiB for an array with shape (1002267, 12673) and data type int32"
     ]
    }
   ],
   "source": [
=======
    "\n",
    "# Assume genres are stored as strings and handle missing or malformed entries\n",
    "def process_genres(genre_list):\n",
    "    if isinstance(genre_list, str):\n",
    "        return genre_list.split(', ')\n",
    "    elif isinstance(genre_list, list):\n",
    "        return genre_list\n",
    "    return []  # Default case if genres are missing or not a string\n",
    "\n",
    "df['genres'] = df['genres'].apply(process_genres)\n",
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "\n",
    "# Preprocess the 'overview' text data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['overview'])\n",
    "\n",
    "# Preprocess the 'genres' categorical data\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_matrix = mlb.fit_transform(df['genres'])\n",
    "\n",
<<<<<<< HEAD
    "# Combine 'overview' TF-IDF features with 'genres' binary features in a sparse format\n",
    "combined_features = hstack([tfidf_matrix, genres_matrix]).tocsr()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = train_test_split(combined_features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the sparse matrix to a Sparse Tensor\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "X_train_tensor = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_test_tensor = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "\n",
    "# Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(X_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy')\n",
    "\n",
    "# Train Model\n",
    "# The 'fit' method can accept sparse tensors as input.\n",
    "model.fit(X_train_tensor, X_train, epochs=5, batch_size=256, verbose=1)\n",
    "\n",
    "# Since we cannot directly use the Keras predict method on sparse tensors,\n",
    "# we define a function that can handle sparse inputs for prediction\n",
    "def predict_with_sparse(model, X_sparse):\n",
    "    # Convert to Sparse Tensor\n",
    "    X_tensor = convert_sparse_matrix_to_sparse_tensor(X_sparse)\n",
    "    # Make prediction\n",
    "    return model.predict(X_tensor, batch_size=256)\n",
    "\n",
    "# Get movie representations from the last trained layer of the model\n",
    "get_layer_output = K.function([model.layers[0].input], [model.layers[-1].output])\n",
    "movie_representations = get_layer_output([X_train_tensor])[0]\n",
    "\n",
    "# Define a function to get the top 5 similar movies using movie_id\n",
    "def recommend_movies(movie_id, movie_representations, df, top_n=5):\n",
    "    # Find the index of the movie_id in df\n",
    "    movie_index = df.index[df['id'] == movie_id].tolist()[0]\n",
    "\n",
    "    # Compute the similarity scores\n",
    "    sim_scores = cosine_similarity(movie_representations[movie_index:movie_index+1], movie_representations)[0]\n",
    "    \n",
    "    # Get the indices of the top_n movies\n",
    "    top_indices = sim_scores.argsort()[-top_n-1:-1][::-1]\n",
    "    \n",
    "    # Return the top_n similar movies not including the input movie itself\n",
    "    return df['title'].iloc[top_indices].tolist()\n",
    "\n",
    "# Example usage: find top 5 movies similar to the movie with ID 27205\n",
    "recommended_movies = recommend_movies(27205, movie_representations, df)\n",
    "print(recommended_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the 'overview' text data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['overview'])\n",
    "\n",
    "# Preprocess the 'genres' categorical data\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_matrix = mlb.fit_transform(df['genres'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenres_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Combine 'overview' TF-IDF features with 'genres' binary features\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#combined_features = np.hstack((tfidf_matrix.toarray(), genres_matrix))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:357\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    353\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_stack_dispatcher)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    Join a sequence of arrays along a new axis.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m \n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mARRAY_FUNCTION_ENABLED:\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;66;03m# raise warning if necessary\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "combined_features = np.hstack([tfidf_matrix, genres_matrix])\n",
    "# Combine 'overview' TF-IDF features with 'genres' binary features\n",
    "#combined_features = np.hstack((tfidf_matrix.toarray(), genres_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Sequential\n",
=======
    "# Combine 'overview' TF-IDF features with 'genres' binary features\n",
    "combined_features = np.hstack((tfidf_matrix.toarray(), genres_matrix))\n",
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = train_test_split(combined_features, test_size=0.2, random_state=42)\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "# Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(X_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train Model\n",
    "# The 'fit' method accepts sparse matrices as input, so there's no need to convert them to dense arrays.\n",
    "model.fit(X_train, X_train, epochs=5, batch_size=256)\n",
    "\n",
=======
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "# Neural Network architecture\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Input Layer\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "\n",
    "# Hidden Layers\n",
    "hidden_layer_1 = Dense(512, activation='relu')(input_layer)\n",
    "dropout_1 = Dropout(0.5)(hidden_layer_1)\n",
    "hidden_layer_2 = Dense(256, activation='relu')(dropout_1)\n",
<<<<<<< HEAD
    "dropout_2 = Dropout(0.5)(hidden_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
    "dropout_2 = Dropout(0.5)(hidden_layer_2)\n",
    "\n",
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "# Output Layer\n",
    "output_layer = Dense(input_shape, activation='sigmoid')(dropout_2)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, X_train, epochs=5, batch_size=256)\n",
    "\n",
    "# Get movie representations\n",
<<<<<<< HEAD
    "movie_representations = model.predict(combined_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
    "movie_representations = model.predict(combined_features)\n",
    "\n",
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
    "# Define a function to get the top 5 similar movies using movie_id\n",
    "def recommend_movies(movie_id, movie_representations, df, top_n=5):\n",
    "    # Find the index of the movie_id in df\n",
    "    movie_index = df.index[df['id'] == movie_id].tolist()[0]\n",
    "\n",
    "    # Compute the similarity scores\n",
    "    sim_scores = cosine_similarity(movie_representations[movie_index:movie_index+1], movie_representations)[0]\n",
    "    \n",
    "    # Get the indices of the top_n movies\n",
    "    top_indices = sim_scores.argsort()[-top_n-1:-1][::-1]\n",
    "    \n",
    "    # Return the top_n similar movies not including the input movie itself\n",
    "    return df['title'].iloc[top_indices].tolist()\n",
    "\n",
    "# Example usage: find top 5 movies similar to the movie with ID 27205\n",
    "recommended_movies = recommend_movies(27205, movie_representations, df)\n",
    "print(recommended_movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.3"
=======
   "version": "3.12.2"
>>>>>>> d60089165363a44a35b2fb90a5ac35298a7a3ad0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
