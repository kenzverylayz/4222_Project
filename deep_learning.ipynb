{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('Merged_df.csv', index_col=0, low_memory=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['MovieID', 'Timestamp', 'Title', 'Genres', 'ZipCode', 'Movie_Title', \n",
    "               'status', 'backdrop_path', 'homepage', 'imdb_id', 'poster_path', 'production_companies', \n",
    "               'production_countries', 'title_year', 'year', 'original_language', \"release_date\", \"original_title\", \"spoken_languages\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows with NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Years into Decades (Buckets to make it a categorical variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert years into decades\n",
    "df1['Decade'] = (df1['Movie_Year'] // 10) * 10  # This floors each year to the start of its decade\n",
    "\n",
    "# Now drop the original 'Movie_Year' column if it's no longer needed\n",
    "df1 = df1.drop(['Movie_Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'adult' column to integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'adult' column to integers (1 for True, 0 for False)\n",
    "df1['adult'] = df1['adult'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert movieids to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['id'] = df1['id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Catgorical features & Scale Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrelim/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical features\n",
    "categorical_features = ['Gender', 'Occupation', 'Age', 'Decade']\n",
    "numerical_features = ['Rating', 'vote_average', 'vote_count', 'revenue', 'runtime', 'budget', 'popularity']\n",
    "\n",
    "# Perform a train-test split\n",
    "df_train, df_test = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-Hot Encoding for categorical variables\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)  # Using sparse=False to get a dense array\n",
    "encoded_features_train = one_hot_encoder.fit_transform(df_train[categorical_features])\n",
    "encoded_features_test = one_hot_encoder.transform(df_test[categorical_features])\n",
    "\n",
    "# Normalize Numerical Features\n",
    "scaler = StandardScaler()\n",
    "scaled_features_train = scaler.fit_transform(df_train[numerical_features])\n",
    "scaled_features_test = scaler.transform(df_test[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Genres into Multiple Binary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since genres are separated by commas in dataset\n",
    "df_train['genres'] = df_train['genres'].apply(lambda x: x.split(', '))\n",
    "df_test['genres'] = df_test['genres'].apply(lambda x: x.split(', '))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit on the training data and transform it\n",
    "genres_encoded_train = mlb.fit_transform(df_train['genres'])\n",
    "\n",
    "# Transform the test data using the same binarizer fitted on the training data\n",
    "genres_encoded_test = mlb.transform(df_test['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before concatenating, Drop all original variables which have been transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training Dataset\n",
    "df_train.drop(categorical_features, axis=1, inplace=True)\n",
    "df_train.drop(numerical_features, axis=1, inplace=True)\n",
    "df_train.drop(\"genres\", axis=1, inplace=True)\n",
    "\n",
    "# For Testing Dataset\n",
    "df_test.drop(categorical_features, axis=1, inplace=True)\n",
    "df_test.drop(numerical_features, axis=1, inplace=True)\n",
    "df_test.drop(\"genres\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all processed features back into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAINING DATASET\n",
    "# Convert encoded categorical features, scaled numerical features, and multiple genere features back to DataFrame for training data\n",
    "encoded_cats_train_df = pd.DataFrame(encoded_features_train, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "scaled_nums_train_df = pd.DataFrame(scaled_features_train, columns=numerical_features)\n",
    "genres_train_df = pd.DataFrame(genres_encoded_train, columns=mlb.classes_)\n",
    "\n",
    "# Reset df_train indexes to match\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine all features\n",
    "df_train_combined = pd.concat([df_train, encoded_cats_train_df, scaled_nums_train_df,genres_train_df], axis=1)\n",
    "\n",
    "# FOR TESTING DATASET\n",
    "# Convert encoded categorical features, scaled numerical features and nultiple genre features back to DataFrame for testing data\n",
    "encoded_cats_test_df = pd.DataFrame(encoded_features_test, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "scaled_nums_test_df = pd.DataFrame(scaled_features_test, columns=numerical_features)\n",
    "genres_test_df = pd.DataFrame(genres_encoded_test, columns=mlb.classes_)\n",
    "\n",
    "# Reset df_test indexes to match\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine all features for testing dataset\n",
    "df_test_combined = pd.concat([df_test, encoded_cats_test_df, scaled_nums_test_df, genres_test_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create User embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = df_train_combined['UserID'].max() + 1  # Add 1 to handle index error as python indexes from 0, then index 0 will be unused/reserved\n",
    "\n",
    "# UsedID embeddings\n",
    "user_id_input = Input(shape=(1,), name='user_id_input')  # Entry point for User IDs  --> Each input is a single value\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=15, name='user_embedding')(user_id_input) # Transforms each userID into a dense vector of size 15 \n",
    "user_embedding_flat = Flatten(name='user_flatten')(user_embedding) # Flatten to a 1D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_features = ['Gender_F', 'Gender_M', 'Occupation_0', 'Occupation_1', 'Occupation_2',\n",
    "                        'Occupation_3', 'Occupation_4', 'Occupation_5', 'Occupation_6',\n",
    "                        'Occupation_7', 'Occupation_8', 'Occupation_9', 'Occupation_10',\n",
    "                        'Occupation_11', 'Occupation_12', 'Occupation_13', 'Occupation_14',\n",
    "                        'Occupation_15', 'Occupation_16', 'Occupation_17', 'Occupation_18',\n",
    "                        'Occupation_19', 'Occupation_20', 'Age_1', 'Age_18', 'Age_25', 'Age_35',\n",
    "                        'Age_45', 'Age_50', 'Age_56']\n",
    "\n",
    "# User demographics input\n",
    "user_demographics_input = Input(shape=(len(demographic_features),), name='user_demographics_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine User embeddings and User demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine User ID embedding with demographic features\n",
    "combined_user_input = concatenate([user_embedding_flat, user_demographics_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Movie Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = df_train_combined['id'].max() + 1 # Add 1 to handle index error as python indexes from 0, then index 0 will be unused/reserved\n",
    "\n",
    "# UsedID embeddings\n",
    "movie_id_input = Input(shape=(1,), name='movie_id_input')  # Entry point for Movie IDs  --> Each input is a single value\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=15, name='movie_embedding')(movie_id_input)\n",
    "movie_flatten = Flatten(name='movie_flatten')(movie_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features_columns = ['vote_average', 'vote_count', 'revenue', 'runtime', 'budget', 'popularity',\n",
    "                          'Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', \n",
    "                          'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery', 'Romance', \n",
    "                          'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western', 'Decade_1910', \n",
    "                          'Decade_1920', 'Decade_1930', 'Decade_1940', 'Decade_1950', 'Decade_1960',\n",
    "                          'Decade_1970', 'Decade_1980', 'Decade_1990', 'Decade_2000']\n",
    "\n",
    "num_movie_features = len(movie_features_columns)  \n",
    "\n",
    "movie_input = Input(shape=(num_movie_features,), name='movie_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user and movie inputs\n",
    "combined_inputs = concatenate([combined_user_input, movie_flatten, movie_input])\n",
    "\n",
    "# Add fully connected layers\n",
    "fc1 = Dense(256, activation='relu')(combined_inputs)\n",
    "dropout1 = Dropout(0.2)(fc1)\n",
    "fc2 = Dense(128, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.2)(fc2)\n",
    "output = Dense(1, activation='linear')(dropout2)\n",
    "\n",
    "# Finalizing the model\n",
    "model = Model(inputs=[user_id_input, user_demographics_input, movie_id_input, movie_input], outputs=output)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error') # Learning rate of 0.001 is a common default for Adam\n",
    "\n",
    "# Model summary to check the architecture\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ratings (Y-values)\n",
    "ratings_train = df_train_combined['Rating'].values\n",
    "ratings_test = df_test_combined['Rating'].values\n",
    "\n",
    "# Prepare inputs for training\n",
    "user_ids_train = df_train_combined['UserID'].values\n",
    "movie_ids_train = df_train_combined['id'].values\n",
    "user_demographics_train = df_train_combined[demographic_features].values\n",
    "movie_features_train = df_train_combined[movie_features_columns].values\n",
    "\n",
    "# Prepare inputs for testing (evaluation)\n",
    "user_ids_test = df_test_combined['UserID'].values\n",
    "movie_ids_test = df_test_combined['id'].values\n",
    "user_demographics_test = df_test_combined[demographic_features].values\n",
    "movie_features_test = df_test_combined[movie_features_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 8484/23811 [=========>....................] - ETA: 8:44 - loss: 0.7152"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([user_ids_train, user_demographics_train, movie_ids_train, movie_features_train], ratings_train, \n\u001b[1;32m      2\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m([user_ids_test, user_demographics_test, movie_ids_test, movie_features_test], ratings_test), \n\u001b[1;32m      3\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      4\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Desktop/NUS/Academics/23:24 SEM 2/BT4222 Mining Web Data for Business Insights/Project/4222_Project/.conda/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([user_ids_train, user_demographics_train, movie_ids_train, movie_features_train], ratings_train, \n",
    "                    validation_data=([user_ids_test, user_demographics_test, movie_ids_test, movie_features_test], ratings_test), \n",
    "                    epochs=10, \n",
    "                    batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
