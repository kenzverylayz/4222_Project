{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenzverylayz/4222_Project/blob/collaborative/deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-1IJhI_jZUJ"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oSp4slGHjZUL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlIeKYuPmroc",
        "outputId": "90c64967-f389-48a2-80d2-e2d04273c6d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wQiMC67jZUL"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bTIyW7KvjZUM"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df = pd.read_csv('Merged_df.csv', index_col=0, low_memory=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpqVspejZUM"
      },
      "source": [
        "Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rxUjuQb4jZUM"
      },
      "outputs": [],
      "source": [
        "df1 = df.drop(['MovieID', 'Timestamp', 'Title', 'Genres', 'ZipCode', 'Movie_Title',\n",
        "               'status', 'backdrop_path', 'homepage', 'imdb_id', 'poster_path', 'production_companies',\n",
        "               'production_countries', 'title_year', 'year', 'original_language', \"release_date\", \"original_title\", \"spoken_languages\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-eheHdXjZUM"
      },
      "source": [
        "Drop rows with NA values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sBRjsJPBjZUN"
      },
      "outputs": [],
      "source": [
        "df1.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6dojFZHjZUN"
      },
      "source": [
        "Converting Years into Decades (Buckets to make it a categorical variable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0hI1XiSjjZUN"
      },
      "outputs": [],
      "source": [
        "# Convert years into decades\n",
        "df1['Decade'] = (df1['Movie_Year'] // 10) * 10  # This floors each year to the start of its decade\n",
        "\n",
        "# Now drop the original 'Movie_Year' column if it's no longer needed\n",
        "df1 = df1.drop(['Movie_Year'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2sD6QgsjZUN"
      },
      "source": [
        "Convert 'adult' column to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "e2OdPUROjZUO"
      },
      "outputs": [],
      "source": [
        "# Convert 'adult' column to integers (1 for True, 0 for False)\n",
        "df1['adult'] = df1['adult'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnJ5SkY1jZUO"
      },
      "source": [
        "Convert movieids to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yUY83sg5jZUO"
      },
      "outputs": [],
      "source": [
        "df1['id'] = df1['id'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8FuTYyjZUO"
      },
      "source": [
        "Encode Catgorical features & Scale Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oaYr9cvBjZUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a243e3c-b5bc-4651-c239-a2ef0be5548e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define categorical and numerical features\n",
        "categorical_features = ['Gender', 'Occupation', 'Age', 'Decade']\n",
        "numerical_features = ['Rating', 'vote_average', 'vote_count', 'revenue', 'runtime', 'budget', 'popularity']\n",
        "\n",
        "# Perform a train-test split\n",
        "df_train, df_test = train_test_split(df1, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)  # Using sparse=False to get a dense array\n",
        "encoded_features_train = one_hot_encoder.fit_transform(df_train[categorical_features])\n",
        "encoded_features_test = one_hot_encoder.transform(df_test[categorical_features])\n",
        "\n",
        "# Normalize Numerical Features\n",
        "scaler = StandardScaler()\n",
        "scaled_features_train = scaler.fit_transform(df_train[numerical_features])\n",
        "scaled_features_test = scaler.transform(df_test[numerical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDJ84J6mjZUQ"
      },
      "source": [
        "Convert Genres into Multiple Binary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ka_VEzPFjZUQ"
      },
      "outputs": [],
      "source": [
        "# Since genres are separated by commas in dataset\n",
        "df_train['genres'] = df_train['genres'].apply(lambda x: x.split(', '))\n",
        "df_test['genres'] = df_test['genres'].apply(lambda x: x.split(', '))\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "# Fit on the training data and transform it\n",
        "genres_encoded_train = mlb.fit_transform(df_train['genres'])\n",
        "\n",
        "# Transform the test data using the same binarizer fitted on the training data\n",
        "genres_encoded_test = mlb.transform(df_test['genres'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WngTrDncjZUQ"
      },
      "source": [
        "Before concatenating, Drop all original variables which have been transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hWEOuk0MjZUQ"
      },
      "outputs": [],
      "source": [
        "# For Training Dataset\n",
        "df_train.drop(categorical_features, axis=1, inplace=True)\n",
        "df_train.drop(numerical_features, axis=1, inplace=True)\n",
        "df_train.drop(\"genres\", axis=1, inplace=True)\n",
        "\n",
        "# For Testing Dataset\n",
        "df_test.drop(categorical_features, axis=1, inplace=True)\n",
        "df_test.drop(numerical_features, axis=1, inplace=True)\n",
        "df_test.drop(\"genres\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0NQ_BOHjZUQ"
      },
      "source": [
        "Combine all processed features back into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JutCtMiSjZUR"
      },
      "outputs": [],
      "source": [
        "# FOR TRAINING DATASET\n",
        "# Convert encoded categorical features, scaled numerical features, and multiple genere features back to DataFrame for training data\n",
        "encoded_cats_train_df = pd.DataFrame(encoded_features_train, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
        "scaled_nums_train_df = pd.DataFrame(scaled_features_train, columns=numerical_features)\n",
        "genres_train_df = pd.DataFrame(genres_encoded_train, columns=mlb.classes_)\n",
        "\n",
        "# Reset df_train indexes to match\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Combine all features\n",
        "df_train_combined = pd.concat([df_train, encoded_cats_train_df, scaled_nums_train_df,genres_train_df], axis=1)\n",
        "\n",
        "# FOR TESTING DATASET\n",
        "# Convert encoded categorical features, scaled numerical features and nultiple genre features back to DataFrame for testing data\n",
        "encoded_cats_test_df = pd.DataFrame(encoded_features_test, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
        "scaled_nums_test_df = pd.DataFrame(scaled_features_test, columns=numerical_features)\n",
        "genres_test_df = pd.DataFrame(genres_encoded_test, columns=mlb.classes_)\n",
        "\n",
        "# Reset df_test indexes to match\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Combine all features for testing dataset\n",
        "df_test_combined = pd.concat([df_test, encoded_cats_test_df, scaled_nums_test_df, genres_test_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJmpt7fHjZUR"
      },
      "source": [
        "Create User embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jiQc8s94jZUR"
      },
      "outputs": [],
      "source": [
        "num_users = df_train_combined['UserID'].max() + 1  # Add 1 to handle index error as python indexes from 0, then index 0 will be unused/reserved\n",
        "\n",
        "# UsedID embeddings\n",
        "user_id_input = Input(shape=(1,), name='user_id_input')  # Entry point for User IDs  --> Each input is a single value\n",
        "user_embedding = Embedding(input_dim=num_users, output_dim=15, name='user_embedding')(user_id_input) # Transforms each userID into a dense vector of size 15\n",
        "user_embedding_flat = Flatten(name='user_flatten')(user_embedding) # Flatten to a 1D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdNIqZitjZUS"
      },
      "source": [
        "User demographics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NKc2ph5rjZUS"
      },
      "outputs": [],
      "source": [
        "demographic_features = ['Gender_F', 'Gender_M', 'Occupation_0', 'Occupation_1', 'Occupation_2',\n",
        "                        'Occupation_3', 'Occupation_4', 'Occupation_5', 'Occupation_6',\n",
        "                        'Occupation_7', 'Occupation_8', 'Occupation_9', 'Occupation_10',\n",
        "                        'Occupation_11', 'Occupation_12', 'Occupation_13', 'Occupation_14',\n",
        "                        'Occupation_15', 'Occupation_16', 'Occupation_17', 'Occupation_18',\n",
        "                        'Occupation_19', 'Occupation_20', 'Age_1', 'Age_18', 'Age_25', 'Age_35',\n",
        "                        'Age_45', 'Age_50', 'Age_56']\n",
        "\n",
        "# User demographics input\n",
        "user_demographics_input = Input(shape=(len(demographic_features),), name='user_demographics_input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Vn6eh5jZUS"
      },
      "source": [
        "Combine User embeddings and User demographics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hRGbj2zPjZUS"
      },
      "outputs": [],
      "source": [
        "# Combine User ID embedding with demographic features\n",
        "combined_user_input = concatenate([user_embedding_flat, user_demographics_input])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuPOGFkVjZUS"
      },
      "source": [
        "Create Movie Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-mypFnSLjZUS"
      },
      "outputs": [],
      "source": [
        "num_movies = df_train_combined['id'].max() + 1 # Add 1 to handle index error as python indexes from 0, then index 0 will be unused/reserved\n",
        "\n",
        "# UsedID embeddings\n",
        "movie_id_input = Input(shape=(1,), name='movie_id_input')  # Entry point for Movie IDs  --> Each input is a single value\n",
        "movie_embedding = Embedding(input_dim=num_movies, output_dim=15, name='movie_embedding')(movie_id_input)\n",
        "movie_flatten = Flatten(name='movie_flatten')(movie_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoeXuRg6jZUS"
      },
      "source": [
        "Movie Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qTuvu9k-jZUS"
      },
      "outputs": [],
      "source": [
        "movie_features_columns = ['vote_average', 'vote_count', 'revenue', 'runtime', 'budget', 'popularity',\n",
        "                          'Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
        "                          'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery', 'Romance',\n",
        "                          'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western', 'Decade_1910',\n",
        "                          'Decade_1920', 'Decade_1930', 'Decade_1940', 'Decade_1950', 'Decade_1960',\n",
        "                          'Decade_1970', 'Decade_1980', 'Decade_1990', 'Decade_2000']\n",
        "\n",
        "num_movie_features = len(movie_features_columns)\n",
        "\n",
        "movie_input = Input(shape=(num_movie_features,), name='movie_input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rm2DFVUjZUS"
      },
      "source": [
        "Assemble Complete Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zrRLq0DhjZUS"
      },
      "outputs": [],
      "source": [
        "# Combine user and movie inputs\n",
        "combined_inputs = concatenate([combined_user_input, movie_flatten, movie_input])\n",
        "\n",
        "# Add fully connected layers\n",
        "fc1 = Dense(256, activation='relu')(combined_inputs)\n",
        "dropout1 = Dropout(0.2)(fc1)\n",
        "fc2 = Dense(128, activation='relu')(dropout1)\n",
        "dropout2 = Dropout(0.2)(fc2)\n",
        "output = Dense(1, activation='linear')(dropout2)\n",
        "\n",
        "# Finalizing the model\n",
        "model = Model(inputs=[user_id_input, user_demographics_input, movie_id_input, movie_input], outputs=output)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer=Adam(0.001), loss='mean_squared_error') # Learning rate of 0.001 is a common default for Adam\n",
        "\n",
        "# Model summary to check the architecture\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0oD9pXBjZUS"
      },
      "source": [
        "Preparing Inputs for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "R7v_6yuyjZUT"
      },
      "outputs": [],
      "source": [
        "# Extract ratings (Y-values)\n",
        "ratings_train = df_train_combined['Rating'].values\n",
        "ratings_test = df_test_combined['Rating'].values\n",
        "\n",
        "# Prepare inputs for training\n",
        "user_ids_train = df_train_combined['UserID'].values\n",
        "movie_ids_train = df_train_combined['id'].values\n",
        "user_demographics_train = df_train_combined[demographic_features].values\n",
        "movie_features_train = df_train_combined[movie_features_columns].values\n",
        "\n",
        "# Prepare inputs for testing (evaluation)\n",
        "user_ids_test = df_test_combined['UserID'].values\n",
        "movie_ids_test = df_test_combined['id'].values\n",
        "user_demographics_test = df_test_combined[demographic_features].values\n",
        "movie_features_test = df_test_combined[movie_features_columns].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od9ffSyXjZUT"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fw9-POLujZUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ff69c0-0534-482e-e206-089bb9273499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23811/23811 [==============================] - 168s 7ms/step - loss: 0.6763 - val_loss: 0.6378\n",
            "Epoch 2/10\n",
            "23811/23811 [==============================] - 163s 7ms/step - loss: 0.6184 - val_loss: 0.6118\n",
            "Epoch 3/10\n",
            "23811/23811 [==============================] - 173s 7ms/step - loss: 0.5924 - val_loss: 0.5990\n",
            "Epoch 4/10\n",
            "23811/23811 [==============================] - 163s 7ms/step - loss: 0.5762 - val_loss: 0.5984\n",
            "Epoch 5/10\n",
            "23811/23811 [==============================] - 172s 7ms/step - loss: 0.5634 - val_loss: 0.5875\n",
            "Epoch 6/10\n",
            "23811/23811 [==============================] - 172s 7ms/step - loss: 0.5533 - val_loss: 0.5866\n",
            "Epoch 7/10\n",
            "23811/23811 [==============================] - 173s 7ms/step - loss: 0.5435 - val_loss: 0.5873\n",
            "Epoch 8/10\n",
            "23811/23811 [==============================] - 167s 7ms/step - loss: 0.5359 - val_loss: 0.5898\n",
            "Epoch 9/10\n",
            "23811/23811 [==============================] - 163s 7ms/step - loss: 0.5275 - val_loss: 0.5826\n",
            "Epoch 10/10\n",
            "23811/23811 [==============================] - 175s 7ms/step - loss: 0.5210 - val_loss: 0.5986\n"
          ]
        }
      ],
      "source": [
        "history = model.fit([user_ids_train, user_demographics_train, movie_ids_train, movie_features_train], ratings_train,\n",
        "                    validation_data=([user_ids_test, user_demographics_test, movie_ids_test, movie_features_test], ratings_test),\n",
        "                    epochs=10,\n",
        "                    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Unseen Movies"
      ],
      "metadata": {
        "id": "gSjkrzT6xzCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract movies yet to be seen by a user\n",
        "def get_unseen_movies(user_id, data = df_train_combined):\n",
        "    seen_movieIDs = data[data['UserID'] == user_id]['id'].unique()\n",
        "    all_movies = data[\"id\"].unique()\n",
        "    unseen_movieIDs = np.setdiff1d(all_movies, seen_movieIDs)\n",
        "    return unseen_movieIDs"
      ],
      "metadata": {
        "id": "sPvuH4tgwbUp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict Ratings with titles for a particular user"
      ],
      "metadata": {
        "id": "1kYZAPa1xx81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ratings_for_user_with_titles(user_id, num_recommendations=5, model = model, data = df_train_combined):\n",
        "\n",
        "    # Extract unseen movie IDs for the user\n",
        "    unseen_movieIDs = get_unseen_movies(user_id, data)\n",
        "\n",
        "    # Extract users demographic features\n",
        "    user_demographics_pred = data[data['UserID'] == user_id][demographic_features].iloc[0].values.reshape(1, -1)\n",
        "    user_demographics_pred = np.repeat(user_demographics_pred, len(unseen_movieIDs), axis=0)\n",
        "\n",
        "    # Extract user IDs and movie IDs\n",
        "    user_id_input = np.array([user_id] * len(unseen_movieIDs))\n",
        "    movie_id_input = unseen_movieIDs\n",
        "\n",
        "    # Prepare movie features for unseen movies\n",
        "    movie_features_df = data[data['id'].isin(unseen_movieIDs)]\n",
        "    # Ensure order matches unseen movies\n",
        "    movie_features_df = movie_features_df.set_index('id').reset_index()\n",
        "    movie_features_input = movie_features_df[movie_features_columns].values\n",
        "\n",
        "    # Predict Ratings for unseen movies\n",
        "    predicted_ratings = model.predict([user_id_input, user_demographics_pred, movie_id_input, movie_features_input]).flatten()\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "\n",
        "    # top_indices = predicted_ratings.argsort()[-num_recommendations:][::-1]\n",
        "    # top_movie_ids = movie_input[top_indices]\n",
        "    # top_predicted_ratings = predicted_ratings[top_indices]\n",
        "    # recommendations_df = pd.DataFrame({\n",
        "    #     'MovieID': top_movie_ids,\n",
        "    #     'PredictedRating': top_predicted_ratings\n",
        "    # })\n",
        "\n",
        "    # recommendations_with_titles = recommendations_df.merge(movie_titles_df, on='MovieID')\n",
        "\n",
        "    # return recommendations_with_titles[['MovieID', 'Title', 'PredictedRating']]\n",
        "\n"
      ],
      "metadata": {
        "id": "PfaQrGXIxwyp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ratings_for_user_with_titles(67)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "LvvoT2Og0U1X",
        "outputId": "090cc747-95c8-48e4-db0b-2d02dbc62976"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous:\n  x sizes: 3028, 3028, 3028, 733565\nMake sure all arrays contain the same number of samples.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-2ad997c78fe7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_ratings_for_user_with_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m67\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-40f2e2d3d3bb>\u001b[0m in \u001b[0;36mpredict_ratings_for_user_with_titles\u001b[0;34m(user_id, num_recommendations, model, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Predict Ratings for unseen movies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpredicted_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_demographics_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_features_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_ratings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3028, 3028, 3028, 733565\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_combined[df_train_combined['UserID'] == 3][demographic_features].iloc[0].values.reshape(1, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjAPHHso0Uyd",
        "outputId": "a5495277-8da5-4802-bb4b-47289b6b79f4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRag-OvN0UmS",
        "outputId": "96fb54e1-87a7-4be9-f396-0c2bf53f7f89"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cac1bc0c910>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}